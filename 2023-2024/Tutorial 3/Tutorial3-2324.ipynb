{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3adf1fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial 3\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: Transformers, Huggingface, Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30acbc3",
   "metadata": {
    "id": "m3wzWLL-LiKd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44dadc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PART 0 ($\\sim$5 mins)\n",
    "*   Downloading a **dataset**.\n",
    "*   Encoding a a **dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f10c45",
   "metadata": {
    "id": "gl48Am5trp3Y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PART I ($\\sim$30 mins)\n",
    "\n",
    "*   Text encoding with transformers.\n",
    "*   Model definition.\n",
    "*   Model training and evaluation with huggingface APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1762696",
   "metadata": {
    "id": "D4anSmM4rp3Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PART II ($\\sim$30 mins)\n",
    "\n",
    "*   Prompting 101\n",
    "*   Sentiment analysis with prompting\n",
    "*   Advanced prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86860b11",
   "metadata": {
    "id": "c4-E45fvrp3Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "First of all, we need to import some useful packages that we will use during this hands-on session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2305467f",
   "metadata": {
    "id": "rUXZLYya69wc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# system packages\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "import sys\n",
    "\n",
    "# data and numerical management packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# useful during debugging (progress bars)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301b767",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch==1.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install transformers==4.30.0\n",
    "!pip install datasets==2.13.2\n",
    "!pip install accelerate -U\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ac9d35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frgg/hf_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e6309a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 10 09:24:44 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   32C    P0    22W /  80W |      8MiB /  8192MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      2234      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c85a3743",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 2560, 'height': 1440, 'scroll': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 2560,\n",
    "        'height': 1440,\n",
    "        'scroll': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef8f7c0",
   "metadata": {
    "id": "qImKj2Mu7LCX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "We will use the IMDB dataset first introduced in tutorial 1.\n",
    "\n",
    "* [**Stats**] A dataset of 50k sentences used for sentiment analysis: 25k with positive sentiment, 25k with negative one.\n",
    "* [**Sentiment**] We consider sentiment labels for classification.\n",
    "\n",
    "We start by **downloading** the dataset and **extract** it to a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "268c2b55",
   "metadata": {
    "id": "NSvqBcKJ7iTY",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(download_path: Path, url: str):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)\n",
    "\n",
    "        \n",
    "def download_dataset(download_path: Path, url: str):\n",
    "    print(\"Downloading dataset...\")\n",
    "    download_url(url=url, download_path=download_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_dataset(download_path: Path, extract_path: Path):\n",
    "    print(\"Extracting dataset... (it may take a while...)\")\n",
    "    with tarfile.open(download_path) as loaded_tar:\n",
    "        loaded_tar.extractall(extract_path)\n",
    "    print(\"Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9d62435",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: /home/frgg/Repositories/nlp-course-material/2023-2024/Tutorial 3\n"
     ]
    }
   ],
   "source": [
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset_name = \"aclImdb\"\n",
    "\n",
    "print(f\"Current work directory: {Path.cwd()}\")\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_tar_path = dataset_folder.joinpath(\"Movies.tar.gz\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_tar_path.exists():\n",
    "    download_dataset(dataset_tar_path, url)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    extract_dataset(dataset_tar_path, dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdbb37a",
   "metadata": {
    "id": "pv3NW1SNrp3a",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data Format\n",
    "\n",
    "Just like in the first assignment, we need a **high level view** of the dataset that is helpful to our needs. \n",
    "\n",
    "We encode the dataset into a [pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9adf0a63",
   "metadata": {
    "id": "P05YfYCe7qCj",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataframe_rows = []\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        folder = dataset_folder.joinpath(dataset_name, split, sentiment)\n",
    "        for file_path in folder.glob('*.txt'):            \n",
    "            with file_path.open(mode='r', encoding='utf-8') as text_file:\n",
    "                text = text_file.read()\n",
    "                score = file_path.stem.split(\"_\")[1]\n",
    "                score = int(score)\n",
    "                file_id = file_path.stem.split(\"_\")[0]\n",
    "\n",
    "                num_sentiment = 1 if sentiment == 'pos' else 0\n",
    "\n",
    "                dataframe_row = {\n",
    "                    \"file_id\": file_id,\n",
    "                    \"score\": score,\n",
    "                    \"sentiment\": num_sentiment,\n",
    "                    \"split\": split,\n",
    "                    \"text\": text\n",
    "                }\n",
    "\n",
    "                dataframe_rows.append(dataframe_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbec9321",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "folder = Path.cwd().joinpath(\"Datasets\", \"Dataframes\", dataset_name)\n",
    "if not folder.exists():\n",
    "    folder.mkdir(parents=True)\n",
    "\n",
    "# transform the list of rows in a proper dataframe\n",
    "df = pd.DataFrame(dataframe_rows)\n",
    "df = df[[\"file_id\", \n",
    "         \"score\",\n",
    "         \"sentiment\",\n",
    "         \"split\",\n",
    "         \"text\"]\n",
    "       ]\n",
    "df_path = folder.with_name(dataset_name + \".pkl\")\n",
    "df.to_pickle(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a185f8",
   "metadata": {
    "id": "Bjf3k-qVrp3b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PART I\n",
    "\n",
    "*   Text encoding with Transformers.\n",
    "*   Model definition.\n",
    "*   Model training and evaluation with huggingface APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a51f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Text encoding with Transformers.\n",
    "\n",
    "In tutorial 1, we have seen how to define standard machine learning models to address sentiment classification.\n",
    "\n",
    "However, we know that Transformer-based models are one of the strongest baselines when assessing a task or benchmarking on a novel corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33067d25",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before defining our transformer-based classifier, we need to encode text inputs into numerical format.\n",
    "\n",
    "As in Tutorial 1, we are going to **tokenize** input texts to perform token indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f720fda",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1 Encoding the dataset\n",
    "\n",
    "First, we are going to use ``datasets`` library to encode our dataset into a handy wrapper for computational speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e20f06bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Slicing for showcasing purposes only!\n",
    "train_df = df.loc[df['split'] == \"train\"].sample(frac=1.0)[:5000]\n",
    "test_df = df.loc[df['split'] == \"test\"].sample(frac=1.0)[:1000]\n",
    "\n",
    "train_data = Dataset.from_pandas(train_df)\n",
    "test_data = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462b9fc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's inspect the newly defined `Dataset` instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbf57dd1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a46d5a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2 Tokenization\n",
    "\n",
    "Transformers typically use [SentencePiece tokenizer](https://github.com/google/sentencepiece) to perform sub-word level tokenization.\n",
    "\n",
    "In particular, the `transformers` library offers the `AutoTokenizer` class to quickly retrieve our chosen transformer's ad-hoc tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44595edb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_card = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8c600",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `model_card` variable defines the *path* where to look for our pre-trained model.\n",
    "\n",
    "You can check [huggingface's hub](https://huggingface.co/models) model hub to pick the model card according to your preference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce3faf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We proceed on tokenizing movie reviews text with our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22969ad0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def preprocess_text(texts):\n",
    "    return tokenizer(texts['text'], truncation=True)\n",
    "\n",
    "train_data = train_data.map(preprocess_text, batched=True)\n",
    "test_data = test_data.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da82c94",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's inspect the preprocess `Dataset` instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0d6b460",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_data) \n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1729211",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2228, 2008, 2026, 5440, 2112, 1997, 2023, 3185, 1010, 1996, 2028, 2008, 4654, 6633, 24759, 14144, 1996, 11591, 23100, 1010, 28072, 1998, 27118, 22758, 1997, 1996, 8931, 1010, 3310, 2012, 1996, 14463, 1997, 1996, 2143, 1012, 3460, 6945, 5912, 1998, 2010, 17204, 2767, 1996, 6458, 2031, 2633, 25878, 1996, 13721, 2158, 2006, 1037, 4899, 2006, 2070, 5108, 1999, 2019, 5992, 11717, 3269, 1012, 2562, 1999, 2568, 2008, 5912, 2038, 2042, 2559, 2005, 1996, 3461, 2005, 3053, 1996, 2972, 2143, 1010, 1998, 2008, 1996, 3461, 2038, 2730, 1998, 8828, 2195, 2111, 2012, 2023, 2391, 1006, 2164, 2010, 5795, 1007, 1010, 1998, 5912, 2003, 2200, 5204, 2008, 3461, 2003, 14196, 9577, 1998, 7501, 2005, 2529, 5771, 1998, 2668, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2061, 1996, 6458, 2038, 2010, 3282, 4197, 2012, 3461, 1010, 2040, 2003, 1010, 1998, 1045, 2507, 1996, 3185, 1998, 6174, 6243, 24387, 2005, 2023, 1010, 1996, 2087, 19424, 1998, 17082, 4874, 1999, 2529, 2433, 2008, 2057, 2031, 2412, 2464, 1012, 1998, 2002, 22114, 1037, 2200, 2590, 3160, 2000, 3460, 6945, 5912, 1024, 1000, 2054, 2079, 2057, 2079, 2085, 1029, 999, 1029, 999, 1029, 1000, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 4950, 7659, 2058, 2000, 3460, 6945, 5912, 1010, 1998, 2009, 1005, 1055, 5793, 2008, 6945, 2038, 2053, 2801, 2054, 2000, 2079, 2279, 1012, 4593, 6945, 2001, 2061, 7848, 2006, 1996, 3291, 1997, 4531, 1996, 13721, 2158, 1010, 2002, 2196, 2245, 2000, 3288, 2247, 2070, 28285, 5733, 1010, 1037, 27333, 2080, 1010, 2030, 11195, 17364, 3388, 1010, 2030, 1037, 5658, 1010, 2030, 2070, 25283, 26147, 17629, 17493, 1010, 2030, 2672, 1037, 2047, 2287, 6823, 2011, 3158, 12439, 2483, 2000, 28384, 1996, 9576, 6841, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2061, 1996, 6458, 6634, 2015, 1998, 11758, 1010, 1996, 13721, 2158, 3632, 2022, 22573, 8024, 1010, 1998, 7632, 8017, 3012, 4372, 6342, 2229, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2672, 2023, 7607, 2339, 9274, 2038, 2042, 29082, 2105, 2007, 1996, 2686, 10382, 2565, 1999, 4942, 1011, 11926, 2686, 2005, 1996, 2197, 2382, 2086, 2612, 1997, 2183, 2067, 2000, 1996, 4231, 2030, 2041, 2000, 7733, 2066, 3071, 4282, 2027, 11276, 2000, 2022, 2725, 1012, 1045, 14145, 2080, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 4312, 1010, 2008, 1005, 1055, 1996, 2785, 1997, 10223, 6508, 1010, 13971, 3015, 1998, 3257, 2008, 2104, 12690, 2015, 2296, 7814, 1997, 2023, 3185, 1012, 2009, 1005, 1055, 2524, 2000, 2360, 2129, 2204, 1996, 5889, 2941, 2024, 1010, 2138, 1996, 3185, 2038, 3143, 17152, 2005, 2037, 3494, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2048, 2060, 11757, 9145, 10071, 2036, 13276, 2039, 1996, 28072, 1997, 1996, 8931, 1024, 2045, 2003, 1037, 3496, 3794, 1996, 15116, 10458, 2214, 3232, 1999, 1996, 2088, 2667, 2000, 8954, 14380, 2015, 2013, 1037, 7676, 1010, 2069, 2000, 2022, 7950, 4237, 2011, 1996, 13721, 2158, 1012, 2023, 3496, 2003, 1037, 23233, 4313, 1999, 17549, 5988, 1012, 1045, 2064, 11302, 2017, 1005, 2310, 2196, 3427, 1037, 2062, 23100, 1998, 29348, 16437, 2007, 5976, 2121, 2559, 2111, 1999, 102]\n"
     ]
    }
   ],
   "source": [
    "print(train_data['input_ids'][50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f9324da",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_data['attention_mask'][50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cbe39b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can perform some quick *sanity check* to evaluate the tokenization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d7c878d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think that my favorite part of this movie, the one that exemplifies the sheer pointless, stupidity and inanity of the proceedings, comes at the climax of the film. DOCTOR TED NELSON and his unmarried friend the Sheriff have finally cornered the Melting Man on a landing on some stairs in an electrical generating plant. Keep in mind that Nelson has been looking for the MM for nearly the entire film, and that the MM has killed and eaten several people at this point (including his boss), and Nelson is very aware that MM is violently insane and hungry for human flesh and blood.<br /><br />So the Sheriff has his gun pointed at MM, who is, and I give the movie and Rick Baker props for this, the most disgusting and terrifying object in human form that we have ever seen. And he yells a very important question to DOCTOR TED NELSON: \"WHAT DO WE DO NOW?!?!?\" <br /><br />The camera cuts over to DOCTOR TED NELSON, and it's obvious that Ted has no idea what to do next. Apparently Ted was so intent on the problem of FINDING the Melting Man, he never thought to bring along some restraining devices, a lasso, or straitjacket, or a net, or some tranquilizer darts, or maybe a New Age tape by Vangelis to soothe the savage beast.<br /><br />So the sheriff panics and shoots, the Melting Man goes berserk, and hilarity ensues. <br /><br />Maybe this explains why NASA has been screwing around with the Space Shuttle program in sub-lunar space for the last 30 years instead of going back to the Moon or out to Mars like everyone knows they OUGHT to be doing. I dunno.<br /><br />Anyway, that's the kind of lousy, lazy writing and direction that undercuts every aspect of this movie. It's hard to say how good the actors actually are, because the movie has complete contempt for their characters.<br /><br />Two other incredibly painful sequences also ramp up the stupidity of the proceedings: There is a scene featuring the lumpiest old couple in the world trying to steal lemons from a grove, only to be torn apart by the Melting Man. This scene is a nadir in 70s cinema. I can guarantee you've never watched a more pointless and irritating setup with odder looking people in your entire life. And the Melting Man's assault on the lady who lives in the house where they keep a horse who pees on the walls defies every attempt to process it.(BTW, I think famous film director Jonathon Demme has a walk-on in this scene as the redneck husband who goes in first to check on the house and never comes out again). The only thing that keeps the actress from literally chewing the scenery is that, as I said, their horse has apparently been peeing on it. And we are forced to watch her hysterics for at least two minutes longer than any SANE film director would hold the shot. <br /><br />Burr DeBenning ought to beat the crap out of IMM's director and photographer. I remember him from an old Columbo episode where he looked MUCH better than he does here - no one's idea of a leading man, but solid and unobtrusive. But no one could possibly be as unappealing in real life as his director makes him look here. <br /><br />Everyone else comes off a little better except for the old couple (and shut up, I know they were being played for laughs, but I ain't laughing!) but not much. <br /><br />This definitely falls into the 'So Bad You Can't Look Away' category of cinema disasters. Still, I'd watch it again before I'd watch a lot of other 70's and 80's abortions ( \"Track of The Moonbeast\" and \"It Lives By Night\" come to mind), and MST's coverage of it is great fun, so if you get a chance, watch the MST version.\n",
      "\n",
      "\n",
      "[CLS] i think that my favorite part of this movie, the one that exemplifies the sheer pointless, stupidity and inanity of the proceedings, comes at the climax of the film. doctor ted nelson and his unmarried friend the sheriff have finally cornered the melting man on a landing on some stairs in an electrical generating plant. keep in mind that nelson has been looking for the mm for nearly the entire film, and that the mm has killed and eaten several people at this point ( including his boss ), and nelson is very aware that mm is violently insane and hungry for human flesh and blood. < br / > < br / > so the sheriff has his gun pointed at mm, who is, and i give the movie and rick baker props for this, the most disgusting and terrifying object in human form that we have ever seen. and he yells a very important question to doctor ted nelson : \" what do we do now?!?!? \" < br / > < br / > the camera cuts over to doctor ted nelson, and it's obvious that ted has no idea what to do next. apparently ted was so intent on the problem of finding the melting man, he never thought to bring along some restraining devices, a lasso, or straitjacket, or a net, or some tranquilizer darts, or maybe a new age tape by vangelis to soothe the savage beast. < br / > < br / > so the sheriff panics and shoots, the melting man goes berserk, and hilarity ensues. < br / > < br / > maybe this explains why nasa has been screwing around with the space shuttle program in sub - lunar space for the last 30 years instead of going back to the moon or out to mars like everyone knows they ought to be doing. i dunno. < br / > < br / > anyway, that's the kind of lousy, lazy writing and direction that undercuts every aspect of this movie. it's hard to say how good the actors actually are, because the movie has complete contempt for their characters. < br / > < br / > two other incredibly painful sequences also ramp up the stupidity of the proceedings : there is a scene featuring the lumpiest old couple in the world trying to steal lemons from a grove, only to be torn apart by the melting man. this scene is a nadir in 70s cinema. i can guarantee you've never watched a more pointless and irritating setup with odder looking people in [SEP]\n"
     ]
    }
   ],
   "source": [
    "original_text = train_data['text'][50]\n",
    "decoded_text = tokenizer.decode(train_data['input_ids'][50])\n",
    "\n",
    "print(original_text)\n",
    "print()\n",
    "print()\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650431ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.3 Vocabulary\n",
    "\n",
    "We **do not** necessarily need to build a vocabulary since transformers already come with their own! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863ee66",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**However**, it is still possible to add new tokens to the vocabulary to adapt the model to the given use case.\n",
    "\n",
    "```\n",
    "tokenizer.add_tokens(new_tokens=new_tokens)\n",
    "```\n",
    "\n",
    "The transformer vocabulary will update its **unusued** vocabulary indexes with newly provided tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b0c55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.4 Special tokens\n",
    "\n",
    "**Pay attention** to used special tokens and their corresponding token ids.\n",
    "\n",
    "Each transformer models has its own special tokens ([CLS], [SEP], [PAD], [EOS], etc...).\n",
    "\n",
    "Thus, the same special token may be mapped to different token ids in distinct transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e203e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.5 Text cleaning\n",
    "\n",
    "We didn't perform any kind of text cleaning before performing text encoding.\n",
    "\n",
    "This is usually because transformer tokenizers **have their own text cleaning process** to perform tokenization.\n",
    "\n",
    "Thus, models **may be sensitive** to custom operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc77cc96",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['couldn', \"'\", 't']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"couldn't\"\n",
    "encoded_example = tokenizer.encode_plus(example_text, add_special_tokens=False)\n",
    "print(encoded_example.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3da65da5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'one', 'point', ',', 'some', 'kids', 'are', 'wandering', 'through', 'the', 'deeper', 'levels', ',', 'exploring', '.']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"At one point,some kids are wandering through the deeper levels, exploring.\"\n",
    "encoded_example = tokenizer.encode_plus(example_text, add_special_tokens=False)\n",
    "print(encoded_example.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8fbc80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "`bert-base-uncased` is trained with text in lower format.\n",
    "\n",
    "**Check model cards** on huggingface to know more about the models you use and inspect their text encoding pipeline to understand how they behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df975a08",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework 📖\n",
    "\n",
    "Experiment with different model cards.\n",
    "\n",
    "Experiment with text cleaning and evaluate its impact on classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331dd42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Model definition\n",
    "\n",
    "We are now ready to define our transformer-based classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf143c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1 Data Formatting\n",
    "\n",
    "We first need to format input data to be fed as mini-batches in a training/evaluation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "955dca33",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba03005b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The ``DataCollatorWithPadding`` receives a batch of\n",
    "\n",
    "```\n",
    "(input_ids, attention_mask, token_type_ids, label)\n",
    "```\n",
    "\n",
    "tuples and **dynamically pads** ``input_ids``, ``attention_mask`` and ``token_type_ids`` to maximum sequence in the batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28deb2da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Intuitively, this operation saves a lot of memory compared to padding to global maximum sequence, while it introduces a reasonable computational overhead.\n",
    "\n",
    "### Note\n",
    "\n",
    "The above example is just one way out of many to perform dynamic batch padding: it really depends on which data structures you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93106b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 Model definition\n",
    "\n",
    "Defining a transformer-based model with huggingface is pretty straightforward!\n",
    "\n",
    "Since we are dealing with text classification, we can use off-the-shelf `AutoModelForSequenceClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a23fd54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_card,\n",
    "                                                           num_labels=2,\n",
    "                                                           id2label={0: 'NEG', 1: 'POS'},\n",
    "                                                           label2id={'NEG': 0, 'POS': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3cacb5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's first check the loaded model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9474b446",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a73ae7c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**That's it!**\n",
    "\n",
    "That's the simplicity of huggingface's APIs.\n",
    "\n",
    "The model is ready to use for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a7dff5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.3 Custom architectures\n",
    "\n",
    "There are plenty of pre-defined model architectures $\\rightarrow$ [auto classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "\n",
    "In more complex scenarios, we may want to define a custom architecture where the pre-trained model is part of it.\n",
    "\n",
    "In these cases, the way you do it strongly depends on the underlying neural library.\n",
    "\n",
    "However, there exist several high-level APIs depending on your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79825da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. Model training and evaluation\n",
    "\n",
    "We are now ready to define the training and evaluation procedures to test our model on the IMDB dataset.\n",
    "\n",
    "In particular, we are going to use ``Trainer`` APIs to efficiently perform training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d60ece",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.1 Metrics\n",
    "\n",
    "First, we define classification metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c352438",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(output_info):\n",
    "    predictions, labels = output_info\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    f1 = f1_score(y_pred=predictions, y_true=labels, average='macro')\n",
    "    acc = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "    return {'f1': f1, 'acc': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf404bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hugginface's metrics\n",
    "\n",
    "Huggingface's offers the **Evaluate** package that contains several evaluation metrics (e.g., accuracy, f1, squad-f1, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f569e08",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "acc_metric = evaluate.load('accuracy')\n",
    "f1_metric = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(output_info):\n",
    "    predictions, labels = output_info\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    return {**f1, **acc}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ffabd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2 Training Arguments\n",
    "\n",
    "The ``Trainer`` object can be extensively customized.\n",
    "\n",
    "Feel free to check the [documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) on training arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba742520",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first rename the `sentiment` column to `label` as the default input to `AutoModelForSequenceClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "632180d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.rename_column('sentiment', 'label')\n",
    "test_data = test_data.rename_column('sentiment', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0c66bf3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_dir\",                 # where to save model\n",
    "    learning_rate=2e-5,                   \n",
    "    per_device_train_batch_size=8,         # accelerate defines distributed training\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",           # when to report evaluation metrics/losses\n",
    "    save_strategy=\"epoch\",                 # when to save checkpoint\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none'                       # disabling wandb (default)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c3afb0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc91d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training schema with collator\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/collator.png\" alt=\"collator\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75174013",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frgg/hf_env/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 02:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.342800</td>\n",
       "      <td>0.266022</td>\n",
       "      <td>0.913900</td>\n",
       "      <td>0.914000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.333054443359375, metrics={'train_runtime': 169.1831, 'train_samples_per_second': 29.554, 'train_steps_per_second': 3.694, 'total_flos': 622869987692928.0, 'train_loss': 0.333054443359375, 'epoch': 1.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904e1d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3 Evaluation\n",
    "\n",
    "We now evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bed901f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "test_prediction_info = trainer.predict(test_data)\n",
    "test_predictions, test_labels = test_prediction_info.predictions, test_prediction_info.label_ids\n",
    "\n",
    "print(test_predictions.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16b09d55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9139004689420969, 'accuracy': 0.914}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = compute_metrics([test_predictions, test_labels])\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ee6e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some cleaning before PART II\n",
    "\n",
    "Let's clean the memory and GPU before switching to instruction-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6898778a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "model = None\n",
    "del model\n",
    "trainer = None\n",
    "del trainer\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5c57b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PART II\n",
    "\n",
    "*   Prompting 101\n",
    "*   Sentiment analysis with prompting\n",
    "*   Advanced prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfbe5ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Prompting 101\n",
    "\n",
    "Prompting is a technique used to adapt a model to a variety of tasks without requiring fine-tuning.\n",
    "\n",
    "```\n",
    "Classify the text into neutral, negative or positive.\n",
    "Text: {text}\n",
    "Sentiment:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088c2cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The model receives the above input prompt and performs text classification via completion.\n",
    "\n",
    "```\n",
    "Classify the text into neutral, negative or positive.\n",
    "Text: {text}\n",
    "Sentiment: {label}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7d59b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In natural language, prompting is a very delicate process since natural language is **expressive**, **flexible**, and, **ambiguous**.\n",
    "\n",
    "A certain concept can be expressed in several ways:\n",
    "\n",
    "* These ways are semantically **equivalent**\n",
    "* May lead to **significant** model performance **drifts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64565dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1 Sensitivity Factors\n",
    "\n",
    "There are two main factors to consider when performing prompt-based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab35a56",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [Prompt Engineering](https://www.promptingguide.ai/)\n",
    "\n",
    "Eventually we have to iteratively find the best performing prompt.\n",
    "\n",
    "This can either done\n",
    "\n",
    "* Manually\n",
    "* Automatically (via an ad-hoc model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b2f34",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [Generation hyper-parameters](https://huggingface.co/docs/transformers/main/generation_strategies#text-generation-strategies)\n",
    "\n",
    "Finding the optimal text generation strategy is a **critical point** for achieving satisfying performance.\n",
    "\n",
    "These strategies affects how the model iteratively selects tokens during generation to avoid phenomena like repetitions, rare words, coherence with input text, and style.\n",
    "\n",
    "* [Deterministic] Greedy $\\rightarrow$ the most preferred (i.e., highest likelihood) token wins\n",
    "* [Deterministic] Beam search\n",
    "* [Stochastic] Top-k sampling\n",
    "* [Stochastic] Nucleus sampling\n",
    "* [Contrastive search](https://huggingface.co/blog/introducing-csearch)  $\\leftarrow$ **recommended**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c7415",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2 Model types\n",
    "\n",
    "There are a lot of different large language models and it is quite easy to be confused.\n",
    "\n",
    "Essentially, we have:\n",
    "\n",
    "* **Base models** (either encoders or encode-decoders): very good at text completion.\n",
    "* **Chat-based models**: base models specifically fine-tuned to address instructions or to chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48705b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "In Huggingface, the distinct is easily formatted as:\n",
    "\n",
    "* `llama2-7b`            $\\rightarrow$ base model\n",
    "* `llama2-7b-*-instruct`   $\\rightarrow$ chat-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b54eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Sentiment analysis with prompting\n",
    "\n",
    "Let's consider our task once again to evaluate prompt-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11b61a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "We first install some package(s) for efficient computation given our hardware limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6012a38c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/timdettmers/bitsandbytes.git\n",
    "!cd bitsandbytes\n",
    "!python setup.py install\n",
    "\n",
    "# Alternatively (Colab) --> restart runtime afterwards! (re-run part I and the first code cell of Part I 3.2)\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed90648",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1 Model pipeline\n",
    "\n",
    "First, we have to define the model pipeline to digest input prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "006b7eda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_card = \"danielhanchen/open_llama_3b_600bt_preview\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_card, load_in_8bit=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a2d11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework 📖\n",
    "\n",
    "Experiment with different model cards (either base or chat-base models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d4350",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 Inference\n",
    "\n",
    "We are now ready to feed prompts to our model and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96d66e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's start with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b34acc8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the text into negative or positive. \n",
      "Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\n",
      "Sentiment:\n",
      "\n",
      "\n",
      "*\n",
      "\n",
      "*Negative:\n"
     ]
    }
   ],
   "source": [
    "def complete_prompt(prompt, **kwargs):\n",
    "    prompt = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "    generated = model.generate(input_ids=prompt['input_ids'],\n",
    "                           attention_mask=prompt['attention_mask'],\n",
    "                           **kwargs)\n",
    "    generated = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "    return generated[0]\n",
    "\n",
    "prompt = \"\"\"Classify the text into negative or positive. \n",
    "Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "generated = complete_prompt(prompt, max_new_tokens=10)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b2be3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we try with the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f14a4c25",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def prepend_prompt(example):\n",
    "    example['prompt'] = formatting_prompt.format(example['text'])\n",
    "    return example\n",
    "    \n",
    "\n",
    "formatting_prompt = \"\"\"Classify the text into negative or positive. \n",
    "Text: {0}\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "test_data = test_data.map(prepend_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0ad2477",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:57<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# first 100 examples for showcasing purposes only (especially given this lazy implementation)\n",
    "generated = [complete_prompt(prompt, max_new_tokens=10) for prompt in tqdm(test_data['prompt'][:100])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80015afc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def parse_generation(text):\n",
    "    label = text.split('Sentiment:')[1].strip()\n",
    "    return [0, 1] if 'positive' in label.casefold() else [1, 0]\n",
    "\n",
    "predictions = [parse_generation(seq) for seq in generated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35a6859c",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.4542619542619543, 'accuracy': 0.58}\n",
      "Classify the text into negative or positive. \n",
      "Text: Good cinematography, good acting good direction...cannot justify a story that is not and cannot be acceptable to any society. Amitabh has often used the media to make this junk sell able by saying that -- if such an incident happens...then what? I would like to ask him if such a thing happens for your own child or your grandchild (say girl child) then what will you do? I think every parents will have to take special care before interacting with any 60 year old neighbor if you have one -jia- with you. Such films should be banned and discouraged otherwise you inspire more more Nithari cases. Such acts are villainous and villains in films are punished..that should be the moral of the story and not glorify their act or them.\n",
      "Sentiment:\n",
      "I am not sure if this is a sentiment or\n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics([np.array(predictions), np.array(test_data['label'][:100])])\n",
    "print(metrics)\n",
    "print(generated[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12d946",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. [Advanced Prompting](https://huggingface.co/docs/transformers/main/tasks/prompting#chain-of-thought)\n",
    "\n",
    "There is no rule of thumb to perform well on prompting.\n",
    "\n",
    "Some may argue it is *art*, some others might say it is just *engineering*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba2402b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, here are some **general recommendations**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c767e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Check **how** the pre-trained model you are using was trained!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfffbcd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Start **simple** and then refine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd63938",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Instructions at the **start/end** of the prompt $\\rightarrow$ based on how most attention layers work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5648a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Separate** input text from instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3bd38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Provide **clear description** of the task: no ambiguity, text format, style, language, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7ce0df",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Evaluate** the prompt on several models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc086cd4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Use advanced techniques: **few-shot prompting**, **Chain-of-thought (CoT)**, Least-to-Most (LtM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96edcd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.1 From Zero- to Few-shot Prompting\n",
    "\n",
    "In many situations, a prompt containing instructions is not sufficient for a model to behave properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e79e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can improve the prompt by providing **a few** ground-truth examples showing how the model should behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bf7da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "Classify the text into negative or positive. \n",
    "Text: {example1}\n",
    "Sentiment: {label1}\n",
    "Text: {example2}\n",
    "Sentiment: {label2}\n",
    "Text: {example3}\n",
    "Sentiment: {label3}\n",
    "Text: {text}\n",
    "Sentiment:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e39bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples may be insufficient\n",
    "\n",
    "Depending on the task at hand, providing examples may be not sufficient for the model to *understand* the instructions.\n",
    "\n",
    "Also, the model might ignore provided examples or it might still perform correctly despite using **intentionally wrong** examples!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60b7487",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Lengthy prompts\n",
    "\n",
    "Adding examples increases the level of detail of prompt, while it may considerably increases its length.\n",
    "\n",
    "Pay attention to what ``model_card`` you choose since your model may **truncate** input prompts!\n",
    "\n",
    "Additionally, a lengthy prompt **increases computation**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60eb03",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples quality\n",
    "\n",
    "Choosing the right set of examples has an impact on model performance.\n",
    "\n",
    "Intuitively, we select examples to maximize (textual) diversity and cover the whole label distribution.\n",
    "\n",
    "In practice, this may be harder than expected: models are sensitive to prompt formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1224ab9a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try sentiment analysis again with Few-shot prompting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8770e83f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def prepend_prompt(example):\n",
    "    example['prompt'] = formatting_prompt.format(example['text'])\n",
    "    return example\n",
    "    \n",
    "\n",
    "formatting_prompt = \"\"\"Classify the text into negative or positive.\n",
    "Text: Everything is so well done: acting, directing, visuals, settings, photography, casting. If you can enjoy a story of real people and real love - this is a winner.\n",
    "Label: positive\n",
    "Text: This is one of the dumbest films, I've ever seen. It rips off nearly ever type of thriller and manages to make a mess of them all.\n",
    "Sentiment: negative\n",
    "Text: {0}\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "test_data = test_data.map(prepend_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7dc3c5f7",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [01:03<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# first 100 examples for showcasing purposes only (especially given this lazy implementation)\n",
    "generated = [complete_prompt(prompt, max_new_tokens=10) for prompt in tqdm(test_data['prompt'][:100])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87dc0ed2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.36305732484076436, 'accuracy': 0.57}\n"
     ]
    }
   ],
   "source": [
    "predictions = [parse_generation(seq) for seq in generated]\n",
    "\n",
    "metrics = compute_metrics([np.array(predictions), np.array(test_data['label'][:100])])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc981549",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework 📖\n",
    "\n",
    "Experiment with different few-shot examples and evaluate corresponding model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c7ee5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2 Chain-of-thought (CoT) Prompting\n",
    "\n",
    "Providing examples to improve task performance may fail in complex scenarios like reasoning tasks.\n",
    "\n",
    "CoT prompting forces the model to generate intermediate reasoning steps before providing the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c05544e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CoT can either be achieved via\n",
    "\n",
    "* Few-shot examples on how to perform *reasoning*\n",
    "* Defining the prompt to force *reasoning* (e.g., *let's think step by step*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad482ca4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try our sentiment analysis task with CoT prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba300273",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def prepend_prompt(example):\n",
    "    example['prompt'] = formatting_prompt.format(example['text'])\n",
    "    return example\n",
    "    \n",
    "\n",
    "formatting_prompt = \"\"\"Classify the text into negative or positive.\n",
    "Text: Everything is so well done: acting, directing, visuals, settings, photography, casting. If you can enjoy a story of real people and real love - this is a winner.\n",
    "Label: positive\n",
    "Text: This is one of the dumbest films, I've ever seen. It rips off nearly ever type of thriller and manages to make a mess of them all.\n",
    "Sentiment: negative\n",
    "Text: {0}\n",
    "Let's think step by step.\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "test_data = test_data.map(prepend_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ec507d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [01:53<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# first 100 examples for showcasing purposes only (especially given this lazy implementation)\n",
    "generated = [complete_prompt(prompt, max_new_tokens=20) for prompt in tqdm(test_data['prompt'][:100])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00181bdc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.36305732484076436, 'accuracy': 0.57}\n"
     ]
    }
   ],
   "source": [
    "predictions = [parse_generation(seq) for seq in generated]\n",
    "\n",
    "metrics = compute_metrics([np.array(predictions), np.array(test_data['label'][:100])])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d420e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework 📖\n",
    "\n",
    "Experiment with different CoT prompts to enforce intermediate reasoning steps.\n",
    "\n",
    "For more details check this [page](https://www.promptingguide.ai/techniques/cot)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5115d1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3 Prompting vs Fine-tuning\n",
    "\n",
    "At last, we may wondering on which technique to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb181f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In short, prompting comes at hand when transferring a pre-trained model on a domain that has some affinities with those seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f4a343",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In other cases like:\n",
    "\n",
    "* Different domain\n",
    "* Sensitive data\n",
    "* Low-resource language\n",
    "* Domain-specific model constraints\n",
    "\n",
    "Fine-tuning is the preferred choice (to maximize improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06465328",
   "metadata": {
    "id": "UbzMCMfprp3m",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
